{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nSimple Linear Regression:\\n\\nsimple linear regression models the relationship between a single independent variable and a dependent variable . it fits a straight line (linear relationship) to the data\\n\\nExample: Suppose you want to predict a person's weight based on their height. Here, height is the independent variable (x), and weight is the dependent variable (y).\\n\\nMultiple Linear Regression:\\n\\nDefinition: Multiple linear regression models the relationship between two or more independent variables and a dependent variable. It fits a linear equation to the data involving multiple predictors.\\n\\nExample: Suppose you want to predict a personâ€™s weight based on their height, age, and gender. Here, height, age, and gender are the independent variables, and weight is the dependent variable.\\n\\n\""
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. Explain the difference between simple linear regression and multiple linear regression. Provide an example of each.\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Simple Linear Regression:\n",
    "\n",
    "simple linear regression models the relationship between a single independent variable and a dependent variable . it fits a straight line (linear relationship) to the data\n",
    "\n",
    "Example: Suppose you want to predict a person's weight based on their height. Here, height is the independent variable (x), and weight is the dependent variable (y).\n",
    "\n",
    "Multiple Linear Regression:\n",
    "\n",
    "Definition: Multiple linear regression models the relationship between two or more independent variables and a dependent variable. It fits a linear equation to the data involving multiple predictors.\n",
    "\n",
    "Example: Suppose you want to predict a person's weight based on their height, age, and gender. Here, height, age, and gender are the independent variables, and weight is the dependent variable.\n",
    "\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.Discuss the assumptions of linear regression. How can you check whether these assumptions hold in a given dataset?\n",
    "\n",
    "\n",
    "\"\"\" \n",
    "Linearity:\n",
    "\n",
    "    Assumption: The relationship between the independent variables and the dependent variable is linear.\n",
    "    \n",
    "    Check:\n",
    "\n",
    "    Use scatter plots or residual plots.\n",
    "    Check linearity assumption visually.\n",
    "\n",
    "Independence:\n",
    "\n",
    "    Assumption: Observations are independent of each other, meaning that the residuals (errors) are not correlated with each other.\n",
    "\n",
    "    Check:\n",
    "    Use Durbin-Watson test for time-series data.\n",
    "    Assess design and context of the data collection to ensure independence\n",
    "    \n",
    "Homoscedasticity:\n",
    "    \n",
    "    Assumption: The variance of the residuals (errors) is constant across all levels of the independent variables. This means that the spread of the residuals should be roughly the same for all predicted values.\n",
    "\n",
    "    Check:\n",
    "\n",
    "    Plot residuals versus fitted values.\n",
    "    Conduct Breusch-Pagan test or similar tests.\n",
    "\n",
    "Normality of Residuals:\n",
    "\n",
    "    Assumption: The residuals (errors) are normally distributed. This is important for making valid inferences about the model coefficients\n",
    "\n",
    "    Check:\n",
    "\n",
    "    Use Q-Q plots and histograms of residuals.\n",
    "    Conduct normality tests like Shapiro-Wilk.\n",
    "\n",
    "\n",
    "No Multicollinearity (for Multiple Linear Regression):\n",
    "\n",
    "    Check:\n",
    "\n",
    "    Calculate VIF for each predictor.\n",
    "    Review the correlation matrix of the independent variables.\n",
    "\n",
    "\n",
    " \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. How do you interpret the slope and intercept in a linear regression model? Provide an example using a real-world scenario.\n",
    "\n",
    "\"\"\"\n",
    "In a linear regression model, the slope and intercept are key components of the regression equation, which typically takes the form:\n",
    "\n",
    "y = b0 + b1x \n",
    "\n",
    "where:\n",
    "\n",
    "y = dependent variable (outcome)\n",
    "x = independent variable (predictor)\n",
    "b0 = intercept (constant term)\n",
    "b1 = slope (coefficient of the predictor)\n",
    "\n",
    "Interpretation of the Slope (b1):\n",
    "\n",
    "    Slope: The slope represents the change in the dependent variable y for a one-unit change in the independent variable x, assuming all other variables remain constant.\n",
    "\n",
    "If b1 is positive, y increases as x increases.\n",
    "If b1 is negative, y decreases as x increases.\n",
    "\n",
    "\n",
    "Interpretation of the Intercept (b0):\n",
    "\n",
    "    Intercept: The intercept represents the value of the dependent variable y when the independent variable x is zero. It is the point where the regression line crosses the y-axis.\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Explain the concept of gradient descent. How is it used in machine learning?\n",
    "\n",
    "\"\"\"\n",
    "Gradient descent is an optimization algorithm commonly used in machine learning to minimize the cost function, which measures the difference between the predicted and actual values. The goal is to find the optimal set of parameters (weights) for a model that minimizes\n",
    "\n",
    "\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Describe the multiple linear regression model. How does it differ from simple linear regression?\n",
    "\n",
    "\n",
    "# Multiple Linear Regression (MLR) is an extension of simple linear regression that allows for the inclusion of multiple independent variables to predict a dependent variable.\n",
    "\n",
    "# y = beta_0 + beta_1 x_1 + beta_2 x_2 + ldots + beta_n x_n + epsilon\n",
    "\n",
    "# simple linear regression involve only one independent variable to predict where as multiple linear regression take more then one independent variable \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explain the concept of multicollinearity in multiple linear regression. How can you detect and address this issue?\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "Multicollinearity occurs when two or more independent variables in a multiple regression model are highly correlated. This means that they provide redundant information about the dependent variable, making it difficult to isolate the individual effect of each predictor on the outcome. High multicollinearity can lead to unstable estimates of regression coefficients, making it hard to determine the true relationship between predictors and the dependent variable.\n",
    "\n",
    "Detecting Multicollinearity:\n",
    "\n",
    "1.Correlation Matrix: Check the correlation matrix of the predictor variables. High correlation coefficients (close to 1 or -1) indicate potential multicollinearity.\n",
    "\n",
    "2.Variance Inflation Factor (VIF): Calculate the VIF for each predictor. A VIF value greater than 10 is often considered indicative of high multicollinearity.\n",
    "\n",
    "3.Tolerance: Tolerance is the reciprocal of VIF. A tolerance value less than 0.1 indicates high multicollinearity.\n",
    "\n",
    "Addressing Multicollinearity:\n",
    "1.Remove Highly Correlated Predictors: Identify and remove one of the highly correlated variables to reduce redundancy.\n",
    "\n",
    "2.Combine Predictors: Create a new predictor by combining the highly correlated variables, such as taking their average or sum.\n",
    "\n",
    "3.Principal Component Analysis (PCA): Transform the original variables into a smaller set of uncorrelated components.\n",
    "\n",
    "4.Regularization Techniques: Use techniques like Ridge Regression or Lasso Regression, which can handle multicollinearity by adding a penalty to the regression coefficients\n",
    "\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Describe the polynomial regression model. How is it different from linear regression?\n",
    "\n",
    "\"\"\"\n",
    "Polynomial Regression is an extension of linear regression that models the relationship between the dependent variable and one or more independent variables as an (n)-th degree polynomial. The general form of the polynomial regression equation is:\n",
    "\n",
    "[ y = beta_0 + beta_1 x + beta_2 x^2 + beta_3 x^3 + ldots + beta_n x^n + epsilon ]\n",
    "\n",
    "\n",
    "Nature of Relationship:\n",
    "\n",
    "a. Linear Regression: Assumes a straight-line relationship between the dependent and independent variables.\n",
    "\n",
    "b. Polynomial Regression: Can model more complex, nonlinear relationships by fitting a polynomial equation to the data\n",
    "\n",
    "Model Complexity:\n",
    "\n",
    "a. Linear Regression: Simpler model with fewer parameters, making it easier to interpret.\n",
    "\n",
    "b. Polynomial Regression: More complex, with higher-degree polynomials potentially leading to overfitting if not properly managed\n",
    "\n",
    "Flexibility:\n",
    "a. Linear Regression: Limited to linear relationships, which may underfit data with nonlinear patterns.\n",
    "\n",
    "b. Polynomial Regression: Offers greater flexibility, capable of modeling curves and intricate patterns in the data\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. What are the advantages and disadvantages of polynomial regression compared to linear regression? In what situations would you prefer to use polynomial regression?\n",
    "\n",
    "\"\"\"\n",
    "Advantages of Polynomial Regression:\n",
    "\n",
    "Flexibility: Polynomial regression can model a wide range of relationships, including nonlinear patterns, which linear regression cannot capture1.\n",
    "Better Fit: It can provide a better fit for data that exhibits curvature, leading to more accurate predictions for such datasets1.\n",
    "\n",
    "Disadvantages of Polynomial Regression:\n",
    "\n",
    "Complexity: Polynomial regression models are more complex and harder to interpret compared to linear regression models1.\n",
    "Overfitting: Higher-degree polynomials can lead to overfitting, where the model fits the training data too closely and performs poorly on new, unseen data1.\n",
    "Computational Cost: More complex models require more computational resources and can be slower to train1.\n",
    "\n",
    "Advantages of Linear Regression:\n",
    "\n",
    "Simplicity: Linear regression is straightforward to implement and interpret, making it easier to understand the relationship between variables1.\n",
    "Efficiency: It requires fewer computational resources and is faster to train compared to polynomial regression\n",
    "Robustness: Linear regression is less prone to overfitting, especially with large datasets1.\n",
    "\n",
    "Disadvantages of Linear Regression:\n",
    "\n",
    "Limited Flexibility: Linear regression cannot model nonlinear relationships, which can lead to underfitting if the data exhibits curvature1.\n",
    "Underfitting: It may not capture the complexity of the data if the true relationship between variables is nonlinear\n",
    "\n",
    "\n",
    "When to Use Polynomial Regression\n",
    "You would prefer to use polynomial regression in the following situations:\n",
    "\n",
    "Nonlinear Relationships: When the relationship between the dependent and independent variables is nonlinear, and a linear model would not capture the complexity of the data1.\n",
    "Curved Patterns: When the data points form a curve, indicating that a higher-degree polynomial might provide a better fit1.\n",
    "Improved Accuracy: When you need a more accurate model that can capture the nuances in the data, and you are willing to manage the increased complexity and risk of overfitting\n",
    "\n",
    "\"\"\"\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
